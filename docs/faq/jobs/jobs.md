---
sidebar_position: 7
sidebar_label: "Job Scheduling"
hide_table_of_contents: true
hide_title: true
pagination_next : null
pagination_prev : null
title: "Job Scheduling"
---

## Job Scheduling

### What is Slurm?

Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for clusters. It facilitates the execution of parallel jobs on the cluster in an efficient manner. For more information on Slurm, users are requested to visit [Slurm Workload Manager - Documentation](https://slurm.schedmd.com/documentation.html).

### What are the frequently used commands for Slurm?

Here is the list of frequently used commands. For more information, users are requested to refer [Slurm Workload Manager -
Documentation](https://slurm.schedmd.com/documentation.html).

-   **salloc** - To allocate resources to a Slurm job with a possible
    set of constraints.

-   **sbatch** - Submits a Slurm job script.

-   **scancel** - Cancels a Slurm job.

-   **scontrol** - To query information and manage jobs.

-   **sinfo** - To retrieve information about partitions and nodes.

-   **squeue** - To query the list of pending and running jobs.

-   **srun** - To run a parallel job on the cluster managed by Slurm.

### How to view the status of partitions?

Users can use the `sinfo` command to view the status of partitions and nodes.

```bash
$ sinfo
```

Alternatively, users can run `scontrol show partition` to know about partitions and their limits.

```bash
$ scontrol show partition
```

### How to view my submitted jobs?

To view all the current jobs of a user, please type the following command.

```bash
$ squeue -u <username>
```

To view all the running jobs of a user, please type

```bash
$ squeue -u <username> -t RUNNING
```

### How to cancel my job?

To cancel a particular job by its `jobid`, use the following command.

```bash
$ scancel <jobid>
```

To cancel a job by its name, please type

```bash
$ scancel --name <jobname>
```

To cancel all the jobs of a user

```bash
$ scancel -u <username>
```

To cancel all the `PENDING` jobs of a user

```bash
$ scancel -t PENDING -u <username>
```

### How to control my job?

To hold a job from being scheduled

```bash
$ scontrol hold <jobid>
```

To release a job to be scheduled

```bash
$ scontrol release <jobid>
```

To requeue (cancel or rerun) a job

```bash
$ scontrol requeue <jobid>
```

For more information, users can use `man scontrol`, `man squeue`,
`man scancel`, etc.

### Can I run my code for a few minutes on the login node?

Users are not authorized to run their codes on the login node. Codes running on login nodes will be terminated automatically. Jobs **have** to be submitted through the scheduler.

### How can I monitor the output of my jobs that are running?

Users can access the output and error logs generated by Slurm while running their codes using the `tail` command. Users are strongly recommended to use the example scripts given above as a base for their job scripts. Job logs are written to the current directory, from which the Slurm job was executed by `sbatch`.

Please note that by default `sbatch` does not write the output logs. The example provided above instructs Slurm to dump the output in the format of `slurm.<jobid>.out`, where `<jobid>` is the job id of the slurm job. Users can use the `tail` command with `-f` flag to view the output file being written continuously by Slurm.

For example, if a user has a job with the job id 121, and would like to view the job output, then use the following syntax.

```bash
$ tail -f slurm.121.out
```

Alternatively, users can use `vi` or `nano` to view the output file after the job is completed.

```bash
$ vi slurm.121.out
$ nano slurm.121.out
```

### Can I run interactive jobs?

Yes, the facility provides users access to interactive job scheduling. Users can use the `srun` command to setup an interactive session on the nodes. Consider an example where a user wants to request 2 nodes with 64 cores per node.

```bash
$ srun -N 2 -n 1 -c 128 -p compute --pty bash -i
```

Here, `-N` is the number of nodes to be used. `-n` is the number of tasks (instances) to be executed. `-c` is the number of cores required for the task. `-p` is the partition to be used. In the present example, we chose the `compute` partition. `--pty bash` instructs Slurm to setup a pseudo terminal in bash. `-i` tells Slurm to run this command in interactive mode.

### I have job scripts written for another job scheduler. Can I use them for Slurm?

No, you cannot execute job scripts written for other job schedulers using Slurm. But, the developers of Slurm have provided users with a documentation containing the correspondences between the options of several job schedulers. Users can access the documentation at this link. [Rosetta stone of Workload manager](https://slurm.schedmd.com/rosetta.pdf).

### When is my job going to start to run?

To get an estimate of when your job is going to start, users can use the `squeue` command.

```bash
$ squeue --start -j <jobid>
```

Please note that your job might run before the scheduled start time as jobs that have finished earlier than their requested walltime might free up the queue and resources needed for your job.

### Why is my job not running?

There are several reasons why your job is not running. Users can run the `squeue` command to get the status and reason.

```bash
$ squeue -j <jobid> -l
```

The `NODELIST(REASON)` section in the output of the above command will have the reason, why Slurm is unable to run the job. Here are the most common reasons.

#### BadConstraints

The job's constraints can not be satisfied.

#### Cleaning

The job is being requeued and still cleaning up from its previous execution.

#### Dependency

This job is waiting for a dependent job to complete.

#### JobHeldAdmin

The job is held by a system administrator. Please contact the system administrator for more information.

#### JobHeldUser

The job is held by the user.

#### NonZeroExitCode

The job terminated with a non-zero exit code.

#### PartitionDown

The partition required by this job is in a DOWN state.

#### Priority

One or more higher priority jobs exist for this partition or advanced reservation.

#### QOSResourceLimit

The job's Quality of Service (QOS) has reached some resource limit.

#### ReqNodeNotAvail

Some node specifically required by the job is not currently available.

#### Resources

The job is waiting for resources to become available.

#### TimeLimit

The job exhausted its time limit.

#### QOSMinCpuNotSatisfied

The job's CPU request doesn't meet the minimum limit of some Quality of Service (QOS).

#### QOSMaxJobsPerUserLimit

The job is unable to run because the user has submitted more jobs of a certain type than are allowed to run at a time.

#### PartitionTimeLimit

The job's time limit exceeds the partition's current time limit.

#### QOSMaxGRESPerJob

The job's GRES request exceeds the maximum each job is allowed to use for the requested Quality of Service (QOS).

#### ReqNodeNotAvail

Some node specifically required by the job is not currently available. If the error message also lists the `UnavailableNodes: ` then it is likely that there is an upcoming reservation or maintenance window on that node.
