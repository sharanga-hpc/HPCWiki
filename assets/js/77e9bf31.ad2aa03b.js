"use strict";(self.webpackChunkhpc_wiki=self.webpackChunkhpc_wiki||[]).push([[3332],{8124:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var r=s(7624),i=s(4552);const o={sidebar_position:3,sidebar_label:"Shared CPU",hide_table_of_contents:!0,hide_title:!0,pagination_next:null,pagination_prev:null,title:"Shared CPU"},t=void 0,l={id:"faq/jobs/shared",title:"Shared CPU",description:"Job Scheduling for shared memory CPU",source:"@site/docs/faq/jobs/shared.md",sourceDirName:"faq/jobs",slug:"/faq/jobs/shared",permalink:"/docs/faq/jobs/shared",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,sidebar_label:"Shared CPU",hide_table_of_contents:!0,hide_title:!0,pagination_next:null,pagination_prev:null,title:"Shared CPU"},sidebar:"FAQ"},a={},d=[{value:"Job Scheduling for shared memory CPU",id:"job-scheduling-for-shared-memory-cpu",level:2},{value:"How to schedule a shared memory CPU parallel job on Slurm?",id:"how-to-schedule-a-shared-memory-cpu-parallel-job-on-slurm",level:3},{value:"Example of a job script <code>job.sh</code> using Modulefiles",id:"example-of-a-job-script-jobsh-using-modulefiles",level:4},{value:"Example of a job script <code>job.sh</code> using Spack",id:"example-of-a-job-script-jobsh-using-spack",level:4}];function c(e){const n={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.M)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"job-scheduling-for-shared-memory-cpu",children:"Job Scheduling for shared memory CPU"}),"\n",(0,r.jsx)(n.h3,{id:"how-to-schedule-a-shared-memory-cpu-parallel-job-on-slurm",children:"How to schedule a shared memory CPU parallel job on Slurm?"}),"\n",(0,r.jsxs)(n.p,{children:["Users can use the ",(0,r.jsx)(n.code,{children:"sbatch"})," command provided by Slurm to submit a job script. Note that, for loading the required packages one can use either ",(0,r.jsx)(n.code,{children:"Modulefiles"})," or ",(0,r.jsx)(n.code,{children:"Spack"}),". In the following we have shown example job scripts using ",(0,r.jsx)(n.code,{children:"Modulefiles"})," and ",(0,r.jsx)(n.code,{children:"Spack"}),"."]}),"\n",(0,r.jsxs)(n.h4,{id:"example-of-a-job-script-jobsh-using-modulefiles",children:["Example of a job script ",(0,r.jsx)(n.code,{children:"job.sh"})," using Modulefiles"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n#SBATCH -p compute\n#SBATCH -N 1\n#SBATCH -c 4\n#SBATCH --mem 512M\n#SBATCH -t 4-2:23 # time (D-HH:MM)\n#SBATCH --job-name="hello_test"\n#SBATCH -o slurm.%j.out\n#SBATCH -e slurm.%j.err\n#SBATCH --mail-user=<username>@hyderabad.bits-pilani.ac.in\n#SBATCH --mail-type=ALL\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nmodule load openmpi-3.1.6-gcc-9.3.0\nmodule load parmetis-4.0.3-gcc-9.3.0\nmodule load openblas-0.3.10-gcc-9.3.0\nmodule load hdf5-1.10.6-gcc-9.3.0\nmodule load petsc-3.13.1-gcc-9.3.0\nsrun ./execname\n'})}),"\n",(0,r.jsx)(n.p,{children:"To submit the above job script use the following command."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ sbatch job.sh\n"})}),"\n",(0,r.jsx)(n.p,{children:"If you wish to test your job script and want to find when it is estimated to run, please run"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ sbatch --test-only job.sh\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Note that this does not actually submit the job. A detailed explanation for each code snippet of the job script ",(0,r.jsx)(n.code,{children:"job.sh"})," is given below."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n"})}),"\n",(0,r.jsx)(n.p,{children:"This is the standard convention to let the linux shell know what interpreter to run."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#SBATCH -p compute\n#SBATCH -N 1\n#SBATCH -c 4\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Configuration variables for Slurm start with ",(0,r.jsx)(n.code,{children:"SBATCH"}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-p"})," refers to the partition to be used by Slurm. Sharanga provides\ntwo partitions, namely, ",(0,r.jsx)(n.code,{children:"compute"})," and ",(0,r.jsx)(n.code,{children:"gpu"}),". For jobs to be executed\nexclusively on CPUs, we use the ",(0,r.jsx)(n.code,{children:"compute"})," partition."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-N"})," represents the number of nodes to be used. In the present\nexample, we are using 1 node. Note that in pure ",(0,r.jsx)(n.code,{children:"OpenMP"}),"\nimplementations, the number of nodes is always set to 1. However,\nif you are using hybrid parallel models such as ",(0,r.jsx)(n.code,{children:"MPI+OpenMP"}),", then\nthe number of nodes can be more than one."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-c"})," represents the number of CPUs per task. For codes employing\nshared memory parallelism, users are requested to specify the number\nof threads as the number of compute cores required."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'#SBATCH --mem 512M\n#SBATCH -t 4-2:23 # time (D-HH:MM)\n#SBATCH --job-name="hello_test"\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"--mem"})," represents the maximum amount of required memory. Here, we\nare requesting 512 Megabytes of memory. Note that Slurm\nprioritises lower memory jobs over higher memory jobs in the queue.\nThis may result in delayed execution of higher memory jobs.\nTherefore, users are requested to give accurate and desirable memory\nlimits."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-t"})," represents the maximum wall clock time the job requires. Here,\nwe are requesting 4 days, 2 hours and 23 minutes. Slurm\nprioritises shorter time limit jobs over longer time limit jobs in\nthe queue. This may result in delayed execution of longer time limit\njobs. Therefore, users are requested to give accurate and desirable\ntime limits. Note that setting values greater than 168 hours will\nresult in the termination of the job by Slurm automatically."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"--job-name"})," represents the name of the job."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#SBATCH -o slurm.%j.out\n#SBATCH -e slurm.%j.err\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-o"})," represents ",(0,r.jsx)(n.code,{children:"stdout"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-e"})," represents ",(0,r.jsx)(n.code,{children:"stderr"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["We are instructing Slurm to redirect ",(0,r.jsx)(n.code,{children:"stdout"})," and ",(0,r.jsx)(n.code,{children:"stderr"})," of the executed application to disk. For example, if your ",(0,r.jsx)(n.code,{children:"jobid"})," is 121, then ",(0,r.jsx)(n.code,{children:"slurm.121.out"})," would contain the normal output of the application, while ",(0,r.jsx)(n.code,{children:"slurm.121.err"})," would contain the error output of the application. These files will be stored in the directory, where the jobs were launched from."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#SBATCH --mail-user=<username>@hyderabad.bits-pilani.ac.in\n#SBATCH --mail-type=ALL\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"\u2013mail-user"})," represents the email address to which job events are to\nbe delivered."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"\u2013mail-type"})," represents the type of events to be logged. Valid type\nvalues are NONE, BEGIN, END, FAIL, REQUEUE, ALL (equivalent to\nBEGIN, END, FAIL, REQUEUE, and STAGE_OUT), STAGE_OUT (burst buffer\nstage out and teardown completed), TIME_LIMIT, TIME_LIMIT_90\n(reached 90 percent of time limit), TIME_LIMIT_80 (reached 80\npercent of time limit) and TIME_LIMIT_50 (reached 50 percent of\ntime limit). Here, we have specified the event type to be ",(0,r.jsx)(n.code,{children:"ALL"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The above command sets the environment variable ",(0,r.jsx)(n.code,{children:"$OMP_NUM_THREADS"}),", which specifies the number of threads to be used in parallel regions of the ",(0,r.jsx)(n.code,{children:"OPENMP"})," code. Here, ",(0,r.jsx)(n.code,{children:"$SLURM_CPUS_PER_TASK"})," is a Slurm defined variable that is automatically set to the value of ",(0,r.jsx)(n.code,{children:"-c"})," directive."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"module load openmpi-3.1.6-gcc-9.3.0\nmodule load parmetis-4.0.3-gcc-9.3.0\nmodule load openblas-0.3.10-gcc-9.3.0\nmodule load hdf5-1.10.6-gcc-9.3.0\nmodule load petsc-3.13.1-gcc-9.3.0\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We are using Modulefiles to set the environment needed to run our application. The example application depends on ",(0,r.jsx)(n.code,{children:"PETSc"})," and also has transitive dependencies ",(0,r.jsx)(n.code,{children:"OpenMPI"}),", ",(0,r.jsx)(n.code,{children:"ParMETIS"}),", ",(0,r.jsx)(n.code,{children:"OpenBLAS"})," and ",(0,r.jsx)(n.code,{children:"HDF5"}),". We are using ",(0,r.jsx)(n.code,{children:"gcc 9.3.0"})," compiled libraries for the application."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"srun ./execname\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Finally, we are using ",(0,r.jsx)(n.code,{children:"srun"})," to start the execution of the application. This is somewhat analogous to ",(0,r.jsx)(n.code,{children:"mpirun"}),". Users are requested ",(0,r.jsx)(n.strong,{children:"not"})," to use ",(0,r.jsx)(n.code,{children:"mpirun"})," and instead use ",(0,r.jsx)(n.code,{children:"srun"}),". ",(0,r.jsx)(n.code,{children:"srun"})," takes care of the allocation and efficient management of resources via Slurm automatically."]}),"\n",(0,r.jsxs)(n.h4,{id:"example-of-a-job-script-jobsh-using-spack",children:["Example of a job script ",(0,r.jsx)(n.code,{children:"job.sh"})," using Spack"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n#SBATCH -p compute\n#SBATCH -N 1\n#SBATCH -c 4\n#SBATCH --mem 512M\n#SBATCH -t 4-2:23 # time (D-HH:MM)\n#SBATCH --job-name="hello_test"\n#SBATCH -o slurm.%j.out\n#SBATCH -e slurm.%j.err\n#SBATCH --mail-user=<username>@hyderabad.bits-pilani.ac.in\n#SBATCH --mail-type=ALL\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nspack load petsc@3.13.1%gcc@9.3.0\nsrun ./execname\n'})}),"\n",(0,r.jsx)(n.p,{children:"To submit the above job script use the following command."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ sbatch job.sh\n"})}),"\n",(0,r.jsx)(n.p,{children:"If you wish to test your job script and want to find when it is estimated to run, please run"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ sbatch --test-only job.sh\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Note that this does not actually submit the job. A detailed explanation for each code snippet of the job script ",(0,r.jsx)(n.code,{children:"job.sh"})," is given below."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\n"})}),"\n",(0,r.jsx)(n.p,{children:"This is the standard convention to let the linux shell know what interpreter to run."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#SBATCH -p compute\n#SBATCH -N 1\n#SBATCH -c 4\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Configuration variables for Slurm start with ",(0,r.jsx)(n.code,{children:"SBATCH"}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-p"})," refers to the partition to be used by Slurm. Sharanga provides\ntwo partitions, namely, ",(0,r.jsx)(n.code,{children:"compute"})," and ",(0,r.jsx)(n.code,{children:"gpu"}),". For jobs to be executed\nexclusively on CPUs, we use the ",(0,r.jsx)(n.code,{children:"compute"})," partition."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-N"})," represents the number of nodes to be used. In the present\nexample, we are using 1 node. Note that in pure ",(0,r.jsx)(n.code,{children:"OpenMP"}),"\nimplementations, the number of nodes is always set to 1. However,\nif you are using hybrid parallel models such as ",(0,r.jsx)(n.code,{children:"MPI+OpenMP"}),", then\nthe number of nodes can be more than one."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-c"})," represents the number of CPUs per task. For codes employing\nshared memory parallelism, users are requested to specify the number\nof threads as the number of compute cores required."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'#SBATCH --mem 512M\n#SBATCH -t 4-2:23 # time (D-HH:MM)\n#SBATCH --job-name="hello_test"\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"--mem"})," represents the maximum amount of required memory. Here, we\nare requesting 512 Megabytes of memory. Note that Slurm\nprioritises lower memory jobs over higher memory jobs in the queue.\nThis may result in delayed execution of higher memory jobs.\nTherefore, users are requested to give accurate and desirable memory\nlimits."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-t"})," represents the maximum wall clock time the job requires. Here,\nwe are requesting 4 days, 2 hours and 23 minutes. Slurm\nprioritises shorter time limit jobs over longer time limit jobs in\nthe queue. This may result in delayed execution of longer time limit\njobs. Therefore, users are requested to give accurate and desirable\ntime limits. Note that setting values greater than 168 hours will\nresult in the termination of the job by Slurm automatically."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"--job-name"})," represents the name of the job."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#SBATCH -o slurm.%j.out\n#SBATCH -e slurm.%j.err\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-o"})," represents ",(0,r.jsx)(n.code,{children:"stdout"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-e"})," represents ",(0,r.jsx)(n.code,{children:"stderr"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["We are instructing Slurm to redirect ",(0,r.jsx)(n.code,{children:"stdout"})," and ",(0,r.jsx)(n.code,{children:"stderr"})," of the executed application to disk. For example, if your ",(0,r.jsx)(n.code,{children:"jobid"})," is 121, then ",(0,r.jsx)(n.code,{children:"slurm.121.out"})," would contain the normal output of the application, while ",(0,r.jsx)(n.code,{children:"slurm.121.err"})," would contain the error output of the application. These files will be stored in the directory, where the jobs were launched from."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#SBATCH --mail-user=<username>@hyderabad.bits-pilani.ac.in\n#SBATCH --mail-type=ALL\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"\u2013mail-user"})," represents the email address to which job events are to\nbe delivered."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"\u2013mail-type"})," represents the type of events to be logged. Valid type\nvalues are NONE, BEGIN, END, FAIL, REQUEUE, ALL (equivalent to\nBEGIN, END, FAIL, REQUEUE, and STAGE_OUT), STAGE_OUT (burst buffer\nstage out and teardown completed), TIME_LIMIT, TIME_LIMIT_90\n(reached 90 percent of time limit), TIME_LIMIT_80 (reached 80\npercent of time limit) and TIME_LIMIT_50 (reached 50 percent of\ntime limit). Here, we have specified the event type to be ",(0,r.jsx)(n.code,{children:"ALL"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The above command sets the environment variable ",(0,r.jsx)(n.code,{children:"$OMP_NUM_THREADS"}),", which specifies the number of threads to be used in parallel regions of the ",(0,r.jsx)(n.code,{children:"OPENMP"})," code. Here, ",(0,r.jsx)(n.code,{children:"$SLURM_CPUS_PER_TASK"})," is a Slurm defined variable that is automatically set to the value of ",(0,r.jsx)(n.code,{children:"-c"})," directive."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"spack load petsc@3.13.1%gcc@9.3.0\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We are using Spack to set the environment needed to run our application. Our example application depends on ",(0,r.jsx)(n.code,{children:"PetSc"})," and has transitive dependencies ",(0,r.jsx)(n.code,{children:"OpenMPI"}),", ",(0,r.jsx)(n.code,{children:"ParMETIS"}),", ",(0,r.jsx)(n.code,{children:"OpenBLAS"})," and ",(0,r.jsx)(n.code,{children:"HDF5"}),". We are using ",(0,r.jsx)(n.code,{children:"gcc 9.3.0"})," compiled libraries for our application. An advantage of Spack over Modulefiles is that Spack handles all the dependencies automatically in one single command."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"srun ./execname\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Finally, we are using ",(0,r.jsx)(n.code,{children:"srun"})," to start the execution of the application. This is somewhat analogous to ",(0,r.jsx)(n.code,{children:"mpirun"}),". Users are requested ",(0,r.jsx)(n.strong,{children:"not"})," to use ",(0,r.jsx)(n.code,{children:"mpirun"})," and instead use ",(0,r.jsx)(n.code,{children:"srun"}),". ",(0,r.jsx)(n.code,{children:"srun"})," takes care of the allocation and efficient management of resources via Slurm automatically."]})]})}function h(e={}){const{wrapper:n}={...(0,i.M)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},4552:(e,n,s)=>{s.d(n,{I:()=>l,M:()=>t});var r=s(1504);const i={},o=r.createContext(i);function t(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);